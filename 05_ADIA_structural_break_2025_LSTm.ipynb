{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìå ADIA Lab Structural Break Challenge  \n\n**Assalam o Alaikumüëã**  \nIn this notebook, we are going to explore the concept of **structural breaks (regime shifts)** ‚Äì basically jab data ka trend, mean, or variance suddenly change ho jaye üìä.  \n\nStructural break detection is an important problem because real-world data kabhi bhi smooth aur stable nahi hota. Kabhi kabhi beech me major shifts aate hain jo forecasting aur analysis dono ko effect kar dete hain.  \n\n---\n\n## üîé Challenge Overview  \n\nWelcome to the **ADIA Lab Structural Break Challenge!**  \nIn this competition, you will analyze **univariate time series data** to determine whether a **structural break** has occurred at a specified boundary point.  \n\n### üìñ What is a Structural Break?  \n\nA **structural break** occurs when the process governing the data generation changes at a certain point in time.  \nThese changes can be subtle or dramatic, and detecting them accurately is crucial across domains:  \n\n- üå¶ **Climatology** ‚Üí shifts in long-term weather patterns  \n- üè≠ **Industrial Monitoring** ‚Üí detecting sudden machine behavior changes  \n- üíπ **Finance** ‚Üí market crashes or regime shifts  \n- üè• **Healthcare** ‚Üí sudden change in patient health indicators  \n\n![Structural Break Example](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/structural-break/quickstarters/baseline/images/example.png)  \n\n---\n\n## üìù Our Task  \n\nFor each time series in the **test set**, we need to predict a **score between `0` and `1`:**  \n\n- `0` ‚Üí No structural break at the specified boundary point  \n- `1` ‚Üí A structural break **did occur**  \n\n---\n\n## üìä Evaluation Metric  \n\nThe challenge uses **ROC AUC (Area Under the Receiver Operating Characteristic Curve)** as the evaluation metric:  \n\n- **ROC AUC ‚âà 0.5** ‚Üí No better than random guessing  \n- **ROC AUC ‚Üí 1.0** ‚Üí Perfect detection performance  \n\nMore about ROC AUC: [sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)  \n\n---\n\n## üìÇ Notebook Flow üöÄ  \n\n1. **Exploratory Data Analysis (EDA)** ‚Äì visualize aur samajhenge dataset.  \n2. **Methods** ‚Äì different techniques (statistical + ML-based) try karenge for break detection.  \n3. **Evaluation** ‚Äì compare karenge results aur dekhenge kaun sa method best perform karta hai.  \n\n---\n\n‚ö° **Goal**: A clean, reproducible, and easy-to-follow Kaggle-style notebook ‚Äì jahan beginner bhi seekh le aur advanced banda bhi enjoy kare.  \n\n**Chalo shuru karte hain üöÄ**  \n","metadata":{}},{"cell_type":"code","source":"# import Important Libraries\n\n!pip install antropy --quiet\n!pip install PyWavelets --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:19:10.362150Z","iopub.execute_input":"2025-09-12T23:19:10.362878Z","iopub.status.idle":"2025-09-12T23:19:16.979588Z","shell.execute_reply.started":"2025-09-12T23:19:10.362851Z","shell.execute_reply":"2025-09-12T23:19:16.978756Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport typing\n\n# Import your dependencies\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport sklearn.metrics\nfrom scipy.stats import wasserstein_distance  # 1D Earth Mover's Distance\nfrom scipy.stats import skew, kurtosis, ks_2samp\nfrom scipy.stats import wasserstein_distance\nfrom sklearn.model_selection import cross_val_score\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report, RocCurveDisplay\nfrom scipy.stats import skew, kurtosis, ks_2samp, mannwhitneyu, wasserstein_distance\nfrom scipy.signal import welch, hilbert\nfrom statsmodels.tsa.stattools import acf, pacf\n\nfrom scipy.signal import welch\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# Ignore UserWarning\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Ignore ConvergenceWarning\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:19:16.980689Z","iopub.execute_input":"2025-09-12T23:19:16.981406Z","iopub.status.idle":"2025-09-12T23:19:23.002910Z","shell.execute_reply.started":"2025-09-12T23:19:16.981376Z","shell.execute_reply":"2025-09-12T23:19:23.002154Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Crunch CLI SETUP","metadata":{}},{"cell_type":"code","source":"%pip install crunch-cli --upgrade --quiet --progress-bar off\n!crunch setup-notebook structural-break 0X4q9UObLYpTP0DOnQ6i6nyK","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:18:57.783428Z","iopub.execute_input":"2025-09-12T23:18:57.783919Z","iopub.status.idle":"2025-09-12T23:19:10.360551Z","shell.execute_reply.started":"2025-09-12T23:18:57.783897Z","shell.execute_reply":"2025-09-12T23:19:10.359780Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\ncrunch-cli, version 7.5.0\nmain.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/29908/main.py (18776 bytes)\nnotebook.ipynb: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/29908/notebook.ipynb (102836 bytes)\nrequirements.txt: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/29908/requirements.original.txt (249 bytes)\ndata/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\ndata/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\ndata/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\ndata/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n                                                                                \n---\nSuccess! Your environment has been correctly setup.\nNext recommended actions:\n1. Load the Crunch Toolings: `crunch = crunch.load_notebook()`\n2. Execute the cells with your code\n3. Run a test: `crunch.test()`\n4. Download and submit your code to the platform!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### BASIC EDA","metadata":{}},{"cell_type":"code","source":"import crunch\n\n# Load the Crunch Toolings\ncrunch = crunch.load_notebook()\n\nX_train, y_train, X_test = crunch.load_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T22:44:26.149315Z","iopub.execute_input":"2025-09-12T22:44:26.149855Z","iopub.status.idle":"2025-09-12T22:44:29.359503Z","shell.execute_reply.started":"2025-09-12T22:44:26.149821Z","shell.execute_reply":"2025-09-12T22:44:29.358916Z"}},"outputs":[{"name":"stdout","text":"loaded inline runner with module: <module '__main__'>\n\ncli version: 7.5.0\navailable ram: 31.35 gb\navailable cpu: 4 core\n----\ndata/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\ndata/X_train.parquet: already exists, file length match\ndata/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\ndata/X_test.reduced.parquet: already exists, file length match\ndata/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\ndata/y_train.parquet: already exists, file length match\ndata/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\ndata/y_test.reduced.parquet: already exists, file length match\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(y_train)\n\nprint(type(X_train))\nprint(type(y_train))\nprint(type(X_test))\n\nprint(type(X_test[0]))\nprint(X_test[0].shape)\ndisplay(X_test[0][:5])\n\n\nX_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T22:44:30.594696Z","iopub.execute_input":"2025-09-12T22:44:30.594967Z","iopub.status.idle":"2025-09-12T22:44:30.624746Z","shell.execute_reply.started":"2025-09-12T22:44:30.594946Z","shell.execute_reply":"2025-09-12T22:44:30.624215Z"}},"outputs":[{"name":"stdout","text":"id\n0        False\n1        False\n2         True\n3        False\n4        False\n         ...  \n9996     False\n9997     False\n9998     False\n9999     False\n10000     True\nName: structural_breakpoint, Length: 10001, dtype: bool\n<class 'pandas.core.frame.DataFrame'>\n<class 'pandas.core.series.Series'>\n<class 'list'>\n<class 'pandas.core.frame.DataFrame'>\n(2779, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"               value  period\nid    time                  \n10001 0     0.010753       0\n      1    -0.031915       0\n      2    -0.010989       0\n      3    -0.011111       0\n      4     0.011236       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>value</th>\n      <th>period</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th>time</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">10001</th>\n      <th>0</th>\n      <td>0.010753</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.031915</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.010989</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.011111</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.011236</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"               value  period\nid    time                  \n0     0    -0.005564       0\n      1     0.003705       0\n      2     0.013164       0\n      3     0.007151       0\n      4    -0.009979       0\n...              ...     ...\n10000 2134  0.001137       1\n      2135  0.003526       1\n      2136  0.000687       1\n      2137  0.001640       1\n      2138  0.001074       1\n\n[23715734 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>value</th>\n      <th>period</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th>time</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th>0</th>\n      <td>-0.005564</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.003705</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.013164</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.007151</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.009979</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">10000</th>\n      <th>2134</th>\n      <td>0.001137</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2135</th>\n      <td>0.003526</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2136</th>\n      <td>0.000687</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2137</th>\n      <td>0.001640</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2138</th>\n      <td>0.001074</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>23715734 rows √ó 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def preprocess_test_data(raw_test_list, start_id=10001):\n    \"\"\"\n    Convert raw test list of arrays to proper MultiIndex DataFrame with fixed index.\n\n    Args:\n        raw_test_list (list of arrays): Raw test data, each element shape (T, 2) with columns (value, period).\n        start_id (int): Starting sample ID for test data indexing (avoid overlap with train IDs).\n\n    Returns:\n        pd.DataFrame: MultiIndex DataFrame with index levels ['id', 'time'].\n    \"\"\"\n    test_list = []\n    for i, ts in enumerate(raw_test_list):\n        df = pd.DataFrame(ts, columns=['value', 'period'])\n        df['id'] = start_id + i\n        df['time'] = df.index\n        df.set_index(['id', 'time'], inplace=True)\n        test_list.append(df)\n\n    X_test_df = pd.concat(test_list)\n\n    # Fix 'time' level if it contains tuples instead of integers\n    new_time_level = [t[1] if isinstance(t, tuple) else t for t in X_test_df.index.get_level_values('time')]\n    new_index = pd.MultiIndex.from_arrays([\n        X_test_df.index.get_level_values('id'),\n        new_time_level,\n    ], names=['id', 'time'])\n    X_test_df.index = new_index\n\n    return X_test_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T22:44:34.833462Z","iopub.execute_input":"2025-09-12T22:44:34.833972Z","iopub.status.idle":"2025-09-12T22:44:34.839827Z","shell.execute_reply.started":"2025-09-12T22:44:34.833948Z","shell.execute_reply":"2025-09-12T22:44:34.839044Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# raw_test_data is the list you get from CrunchDAO or elsewhere\n\nX_test_df = preprocess_test_data(X_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T22:44:38.426229Z","iopub.execute_input":"2025-09-12T22:44:38.426492Z","iopub.status.idle":"2025-09-12T22:44:41.606632Z","shell.execute_reply.started":"2025-09-12T22:44:38.426472Z","shell.execute_reply":"2025-09-12T22:44:41.605941Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\"Train index levels:\", X_train.index.names)\nprint(\"Train index sample:\")\nprint(X_train.head())\n\nprint(\"\\nTest index levels:\", X_test_df.index.names)\nprint(\"Test index sample:\")\nprint(X_test_df.head())\n\nassert X_train.index.names == X_test_df.index.names, \"Index levels mismatch!\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T22:44:41.607936Z","iopub.execute_input":"2025-09-12T22:44:41.608234Z","iopub.status.idle":"2025-09-12T22:44:41.616441Z","shell.execute_reply.started":"2025-09-12T22:44:41.608205Z","shell.execute_reply":"2025-09-12T22:44:41.615792Z"}},"outputs":[{"name":"stdout","text":"Train index levels: ['id', 'time']\nTrain index sample:\n            value  period\nid time                  \n0  0    -0.005564       0\n   1     0.003705       0\n   2     0.013164       0\n   3     0.007151       0\n   4    -0.009979       0\n\nTest index levels: ['id', 'time']\nTest index sample:\n               value  period\nid    time                  \n10001 0     0.010753       0\n      1    -0.031915       0\n      2    -0.010989       0\n      3    -0.011111       0\n      4     0.011236       0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### feature engineering","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.signal import find_peaks\nfrom scipy.stats import entropy\n\ndef extract_features(df, seq_ids, val_col='value'):\n    features_list = []\n\n    for seq_id in seq_ids:\n        seq_vals = df.loc[seq_id][val_col].values.astype(np.float32)\n        mean = np.mean(seq_vals)\n        std = np.std(seq_vals)\n        minimum = np.min(seq_vals)\n        maximum = np.max(seq_vals)\n        median = np.median(seq_vals)\n        range_ = maximum - minimum\n        skewness = pd.Series(seq_vals).skew()\n        kurtosis = pd.Series(seq_vals).kurtosis()\n        energy = np.sum(seq_vals ** 2)\n        # Normalize values to calculate entropy robustly; add small constant to avoid zero counts\n        hist, _ = np.histogram(seq_vals, bins=30, density=True)\n        seq_entropy = entropy(hist + 1e-6)\n        peaks, _ = find_peaks(seq_vals)\n        num_peaks = len(peaks)\n        # Peak distances statistics\n        if len(peaks) > 1:\n            peak_distances = np.diff(peaks)\n            peak_dist_mean = np.mean(peak_distances)\n            peak_dist_std = np.std(peak_distances)\n        else:\n            peak_dist_mean = 0\n            peak_dist_std = 0\n        rms = np.sqrt(np.mean(seq_vals**2))\n\n        features = [\n            mean, std, minimum, maximum, median, range_, skewness, kurtosis,\n            energy, seq_entropy, num_peaks, peak_dist_mean, peak_dist_std, rms\n        ]\n        features_list.append(features)\n\n    features_array = np.array(features_list, dtype=np.float32)\n    return torch.tensor(features_array)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:15:56.515402Z","iopub.execute_input":"2025-09-12T23:15:56.515979Z","iopub.status.idle":"2025-09-12T23:15:56.526403Z","shell.execute_reply.started":"2025-09-12T23:15:56.515947Z","shell.execute_reply":"2025-09-12T23:15:56.525673Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split\nimport numpy as np\nimport pandas as pd\nfrom scipy.signal import find_peaks\nfrom scipy.stats import entropy\n\n# Sequence Dataset\nclass SequenceDataset(Dataset):\n    def __init__(self, df, seq_ids, max_len=500, val_col='value'):\n        self.df = df\n        self.seq_ids = seq_ids\n        self.max_len = max_len\n        self.val_col = val_col\n    def __len__(self):\n        return len(self.seq_ids)\n    def __getitem__(self, idx):\n        seq_id = self.seq_ids[idx]\n        seq_vals = self.df.loc[seq_id][self.val_col].values\n        padded = np.zeros(self.max_len, dtype=np.float32)\n        length = min(len(seq_vals), self.max_len)\n        padded[:length] = seq_vals[:length]\n        return torch.tensor(padded).unsqueeze(-1)\n\n# Embedding Dataset\nclass EmbeddingDataset(Dataset):\n    def __init__(self, embeddings, labels):\n        self.embeddings = embeddings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        return self.embeddings[idx], self.labels[idx]\n\n# Weighted Sampler for Imbalance\ndef create_balanced_sampler(labels):\n    class_sample_counts = torch.tensor([(labels == 0).sum(), (labels == 1).sum()], dtype=torch.float32)\n    weight_per_class = 1.0 / class_sample_counts\n    weights = weight_per_class[labels]\n    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n    return sampler\n\n# Improved LSTM Autoencoder\nclass LSTMAutoencoderImproved(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=64, latent_dim=32, num_layers=2, dropout=0.2):\n        super().__init__()\n        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n        self.enc_fc = nn.Sequential(\n            nn.Linear(hidden_dim, latent_dim),\n            nn.ReLU()\n        )\n        self.dec_fc = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.LSTM(hidden_dim, input_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n    def forward(self, x):\n        enc_out, (h_n, _) = self.encoder(x)\n        h_last = h_n[-1]\n        latent = self.enc_fc(h_last)\n        dec_in = self.dec_fc(latent).unsqueeze(1).repeat(1, x.size(1), 1)\n        dec_out, _ = self.decoder(dec_in)\n        return dec_out, latent\n\n# Enhanced feature extractor based on research\ndef extract_features(df, seq_ids, val_col='value'):\n    features_list = []\n\n    for seq_id in seq_ids:\n        seq_vals = df.loc[seq_id][val_col].values.astype(np.float32)\n        mean = np.mean(seq_vals)\n        std = np.std(seq_vals)\n        minimum = np.min(seq_vals)\n        maximum = np.max(seq_vals)\n        median = np.median(seq_vals)\n        range_ = maximum - minimum\n        skewness = pd.Series(seq_vals).skew()\n        kurtosis = pd.Series(seq_vals).kurtosis()\n        energy = np.sum(seq_vals ** 2)\n        hist, _ = np.histogram(seq_vals, bins=30, density=True)\n        seq_entropy = entropy(hist + 1e-6)\n        peaks, _ = find_peaks(seq_vals)\n        num_peaks = len(peaks)\n        if len(peaks) > 1:\n            peak_distances = np.diff(peaks)\n            peak_dist_mean = np.mean(peak_distances)\n            peak_dist_std = np.std(peak_distances)\n        else:\n            peak_dist_mean = 0\n            peak_dist_std = 0\n        rms = np.sqrt(np.mean(seq_vals**2))\n\n        features = [\n            mean, std, minimum, maximum, median, range_, skewness, kurtosis,\n            energy, seq_entropy, num_peaks, peak_dist_mean, peak_dist_std, rms\n        ]\n        features_list.append(features)\n\n    features_array = np.array(features_list, dtype=np.float32)\n    return torch.tensor(features_array)\n\n# Extract embeddings utility\ndef extract_embeddings(model, dataloader, device):\n    model.to(device)\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = batch.to(device)\n            _, latent = model(batch)\n            embeddings.append(latent.cpu())\n    embeddings = torch.cat(embeddings, dim=0)\n    print(f\"Extracted embeddings shape: {embeddings.shape}\")\n    return embeddings\n\n# Classifier Model\nclass BreakClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, 1)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x.squeeze()\n\n# Create train/validation loaders for sequences\ndef create_train_val_loaders(df, seq_ids, val_fraction=0.1, batch_size=64, max_len=500):\n    total = len(seq_ids)\n    val_size = int(total * val_fraction)\n    train_size = total - val_size\n    train_ids, val_ids = random_split(seq_ids, [train_size, val_size])\n    train_dataset = SequenceDataset(df, train_ids, max_len=max_len)\n    val_dataset = SequenceDataset(df, val_ids, max_len=max_len)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader, train_ids\n\n# Autoencoder training with early stopping and improved optimization\ndef train_autoencoder_with_early_stopping_and_val(\n    model, train_loader, val_loader, device,\n    epochs=50, patience=5, min_delta=1e-4):\n    \n    model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, verbose=True)\n    criterion = nn.MSELoss()\n    \n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            recon, _ = model(batch)\n            loss = criterion(recon, batch)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_train_loss = running_loss / len(train_loader)\n        \n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for val_batch in val_loader:\n                val_batch = val_batch.to(device)\n                recon, _ = model(val_batch)\n                loss = criterion(recon, val_batch)\n                val_loss += loss.item()\n        avg_val_loss = val_loss / len(val_loader)\n        \n        scheduler.step(avg_val_loss)\n        \n        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n        \n        if best_val_loss - avg_val_loss > min_delta:\n            best_val_loss = avg_val_loss\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), 'best_autoencoder.pth')\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f\"Early stopping after {epoch+1} epochs (min_delta={min_delta}).\")\n                break\n    model.load_state_dict(torch.load('best_autoencoder.pth'))\n    print(\"Autoencoder training complete with early stopping.\")\n\n# Classifier training with validation, early stopping, AdamW, and balanced sampler\ndef train_classifier_with_val(\n    model, dataset, device,\n    epochs=20, batch_size=64,\n    patience=5, val_fraction=0.1, min_delta=1e-4):\n    \n    total_len = len(dataset)\n    val_len = int(total_len * val_fraction)\n    train_len = total_len - val_len\n    train_set, val_set = random_split(dataset, [train_len, val_len])\n    \n    train_labels = torch.tensor([label for _, label in train_set])\n    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=create_balanced_sampler(train_labels), shuffle=False)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n    \n    model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5, verbose=True)\n\n    all_labels = torch.tensor([label for _, label in dataset])\n    pos_weight_val = (all_labels == 0).sum() / (all_labels == 1).sum()\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_val.to(device))\n    \n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n        for batch_x, batch_y in train_loader:\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device).float()\n            optimizer.zero_grad()\n            output = model(batch_x)\n            loss = criterion(output, batch_y)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        avg_train_loss = running_loss / len(train_loader)\n        \n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch_x, batch_y in val_loader:\n                batch_x, batch_y = batch_x.to(device), batch_y.to(device).float()\n                output = model(batch_x)\n                loss = criterion(output, batch_y)\n                val_loss += loss.item()\n        avg_val_loss = val_loss / len(val_loader)\n        \n        scheduler.step(avg_val_loss)\n        \n        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.6f} - Val Loss: {avg_val_loss:.6f}\")\n        \n        if best_val_loss - avg_val_loss > min_delta:\n            best_val_loss = avg_val_loss\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), 'best_classifier.pth')\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f\"Early stopping after {epoch+1} epochs (min_delta={min_delta}).\")\n                break\n    model.load_state_dict(torch.load('best_classifier.pth'))\n    print(\"Classifier training complete with early stopping.\")\n\n# Full training pipeline\ndef train(df, seq_ids, labels_dict, device='cpu', max_len=500, ae_epochs=50, clf_epochs=20, batch_size=64):\n    print(\"== Starting training pipeline with validation split and improved optimization ==\")\n    \n    # Create train and validation loaders for autoencoder training\n    train_loader, val_loader, train_ids = create_train_val_loaders(df, seq_ids, val_fraction=0.1, batch_size=batch_size, max_len=max_len)\n    \n    # Initialize and train the improved autoencoder\n    autoencoder = LSTMAutoencoderImproved()\n    print(\"Training Autoencoder with validation and early stopping...\")\n    train_autoencoder_with_early_stopping_and_val(autoencoder, train_loader, val_loader, device=device, epochs=ae_epochs)\n    \n    # Extract embeddings on full dataset after autoencoder training completes\n    print(\"Extracting embeddings from full training set...\")\n    full_ae_dataset = SequenceDataset(df, seq_ids, max_len=max_len)\n    full_ae_loader = DataLoader(full_ae_dataset, batch_size=batch_size, shuffle=False)\n    embeddings = extract_embeddings(autoencoder, full_ae_loader, device=device)\n    \n    # Extract enhanced handcrafted features and combine\n    handcrafted_features = extract_features(df, seq_ids)\n    combined_features = torch.cat([embeddings, handcrafted_features], dim=1)\n    \n    # Prepare classifier dataset and initialize classifier\n    label_list = [labels_dict[id_] for id_ in seq_ids]\n    label_tensor = torch.tensor(label_list, dtype=torch.long)\n    \n    clf_dataset = EmbeddingDataset(combined_features, label_tensor)\n    classifier = BreakClassifier(input_dim=combined_features.shape[1])\n    \n    # Train classifier with validation, early stopping, AdamW, and balancing\n    print(\"Training classifier with validation and early stopping...\")\n    train_classifier_with_val(classifier, clf_dataset, device=device, epochs=clf_epochs, batch_size=batch_size)\n    \n    # Save final models\n    torch.save(autoencoder.state_dict(), \"autoencoder.pth\")\n    torch.save(classifier.state_dict(), \"classifier.pth\")\n    print(\"Models saved successfully.\")\n    print(\"== Training pipeline complete ==\")\n    return autoencoder, classifier\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:18:01.187278Z","iopub.execute_input":"2025-09-12T23:18:01.187876Z","iopub.status.idle":"2025-09-12T23:18:01.217846Z","shell.execute_reply.started":"2025-09-12T23:18:01.187855Z","shell.execute_reply":"2025-09-12T23:18:01.217108Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"- Input: Raw sequence data (X_train) + labels (y_train) per sequence.\n- Feature Engineering: Handcrafted features ya embeddings extraction ke liye preprocessing.\n- Autoencoder Training: Sequence reconstruction ke liye autoencoder train ho.\n- Embeddings Extraction: Encoder se latent embeddings nikalain.\n- Concatenate Features: Handcrafted + embeddings.\n- Oversampling Setup: Minority class oversample karain.\n- Classifier Training: Combined features ke sath break/no-break classifier train ho.\n- Progress Prints: Har step pe status print ho.","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nlabels_dict = y_train.astype(int).to_dict()\n\nautoencoder_model, classifier_model = train(\n    X_train,\n    seq_ids,\n    labels_dict,\n    device=device,\n    max_len=500,\n    ae_epochs=20,\n    clf_epochs=20,\n    batch_size=64\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:09:09.938270Z","iopub.execute_input":"2025-09-12T23:09:09.939093Z","iopub.status.idle":"2025-09-12T23:10:03.249304Z","shell.execute_reply.started":"2025-09-12T23:09:09.939064Z","shell.execute_reply":"2025-09-12T23:10:03.248696Z"}},"outputs":[{"name":"stdout","text":"== Starting training pipeline with validation split ==\nTraining Autoencoder with validation and early stopping...\nEpoch 1/10 - Train Loss: 0.006458 - Val Loss: 0.000622\nEpoch 2/10 - Train Loss: 0.002666 - Val Loss: 0.000619\nEpoch 3/10 - Train Loss: 0.002664 - Val Loss: 0.000619\nEpoch 4/10 - Train Loss: 0.002664 - Val Loss: 0.000619\nEpoch 5/10 - Train Loss: 0.002663 - Val Loss: 0.000619\nEpoch 6/10 - Train Loss: 0.002663 - Val Loss: 0.000619\nEpoch 7/10 - Train Loss: 0.002662 - Val Loss: 0.000619\nEpoch 8/10 - Train Loss: 0.002662 - Val Loss: 0.000619\nEpoch 9/10 - Train Loss: 0.002661 - Val Loss: 0.000619\nEpoch 10/10 - Train Loss: 0.002661 - Val Loss: 0.000619\nAutoencoder training complete with early stopping.\nExtracting embeddings from full training set...\nExtracted embeddings shape: torch.Size([10001, 32])\nTraining classifier with validation and early stopping...\nEpoch 1/10 - Train Loss: 1.068882 - Val Loss: 1.227996\nEpoch 2/10 - Train Loss: 1.050734 - Val Loss: 1.129139\nEpoch 3/10 - Train Loss: 1.051483 - Val Loss: 1.215364\nEpoch 4/10 - Train Loss: 1.053514 - Val Loss: 1.139795\nEpoch 5/10 - Train Loss: 1.055683 - Val Loss: 1.143494\nEpoch 6/10 - Train Loss: 1.058273 - Val Loss: 1.114007\nEpoch 7/10 - Train Loss: 1.042598 - Val Loss: 1.116001\nEpoch 8/10 - Train Loss: 1.045046 - Val Loss: 1.097631\nEpoch 9/10 - Train Loss: 1.041871 - Val Loss: 1.134826\nEpoch 10/10 - Train Loss: 1.032980 - Val Loss: 1.145085\nClassifier training complete with early stopping.\nModels saved successfully.\n== Training pipeline complete ==\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = list(infer(X_test, \"artifacts\"))\nprint(preds[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T20:10:38.898799Z","iopub.execute_input":"2025-09-11T20:10:38.899106Z","iopub.status.idle":"2025-09-11T20:11:10.591577Z","shell.execute_reply.started":"2025-09-11T20:10:38.899081Z","shell.execute_reply":"2025-09-11T20:11:10.590650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"crunch.test(\n    # Uncomment to disable the train\n    #force_first_train=False,\n\n    # Uncomment to disable the determinism check\n    # no_determinism_check=True,\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T20:11:21.555562Z","iopub.execute_input":"2025-09-11T20:11:21.555858Z","iopub.status.idle":"2025-09-11T20:33:00.908901Z","shell.execute_reply.started":"2025-09-11T20:11:21.555838Z","shell.execute_reply":"2025-09-11T20:33:00.908168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction = pd.read_parquet(\"data/prediction.parquet\")\n\nprediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T20:33:00.910253Z","iopub.execute_input":"2025-09-11T20:33:00.910547Z","iopub.status.idle":"2025-09-11T20:33:00.923165Z","shell.execute_reply.started":"2025-09-11T20:33:00.910526Z","shell.execute_reply":"2025-09-11T20:33:00.922323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the targets\ntarget = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n\n# Call the scoring function\nsklearn.metrics.roc_auc_score(\n    target,\n    prediction,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T20:33:00.924258Z","iopub.execute_input":"2025-09-11T20:33:00.924587Z","iopub.status.idle":"2025-09-11T20:33:00.943644Z","shell.execute_reply.started":"2025-09-11T20:33:00.924556Z","shell.execute_reply":"2025-09-11T20:33:00.942784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T20:34:17.601679Z","iopub.execute_input":"2025-09-11T20:34:17.602008Z","iopub.status.idle":"2025-09-11T20:34:17.610170Z","shell.execute_reply.started":"2025-09-11T20:34:17.601988Z","shell.execute_reply":"2025-09-11T20:34:17.609240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}