{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install antropy --quiet\n!pip install PyWavelets --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T18:34:40.902668Z","iopub.execute_input":"2025-09-05T18:34:40.903016Z","iopub.status.idle":"2025-09-05T18:34:49.227505Z","shell.execute_reply.started":"2025-09-05T18:34:40.902988Z","shell.execute_reply":"2025-09-05T18:34:49.225726Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"%pip install crunch-cli --upgrade --quiet --progress-bar off\n!crunch setup-notebook structural-break vpZXd9oUlwMy6GhLU36kHqPf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7DUeixiC_IJM","outputId":"acc1a734-d506-4a0f-b234-59436c8d7683","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T18:35:19.971645Z","iopub.execute_input":"2025-09-05T18:35:19.972021Z","iopub.status.idle":"2025-09-05T18:35:29.297954Z","shell.execute_reply.started":"2025-09-05T18:35:19.971988Z","shell.execute_reply":"2025-09-05T18:35:29.296604Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\ncrunch-cli, version 7.4.0\ndelete /kaggle/working/.crunchdao\nyou appear to have never submitted code before\ndata/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\ndata/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\ndata/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\ndata/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n                                                                                \n---\nSuccess! Your environment has been correctly setup.\nNext recommended actions:\n1. Load the Crunch Toolings: `crunch = crunch.load_notebook()`\n2. Execute the cells with your code\n3. Run a test: `crunch.test()`\n4. Download and submit your code to the platform!\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"EpLeMWSw-0fQ"}},{"cell_type":"code","source":"import os\nimport typing\n\n# Import your dependencies\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport sklearn.metrics\nfrom scipy.stats import wasserstein_distance  # 1D Earth Mover's Distance\n\nimport warnings\nfrom scipy.signal import welch\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"ExecuteTime":{"end_time":"2024-11-18T09:52:21.302334Z","start_time":"2024-11-18T09:52:18.268241Z"},"id":"MKqz-6Zw-0fR","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:32:15.129994Z","iopub.execute_input":"2025-09-05T19:32:15.130967Z","iopub.status.idle":"2025-09-05T19:32:15.137680Z","shell.execute_reply.started":"2025-09-05T19:32:15.130922Z","shell.execute_reply":"2025-09-05T19:32:15.136491Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"import crunch\n\n# Load the Crunch Toolings\ncrunch = crunch.load_notebook()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xjD_WSAS-0fR","outputId":"75b022d0-b3ae-4c07-e136-8ea22537731f","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:32:20.926991Z","iopub.execute_input":"2025-09-05T19:32:20.927379Z","iopub.status.idle":"2025-09-05T19:32:20.933625Z","shell.execute_reply.started":"2025-09-05T19:32:20.927353Z","shell.execute_reply":"2025-09-05T19:32:20.932641Z"}},"outputs":[{"name":"stdout","text":"loaded inline runner with module: <module '__main__'>\n\ncli version: 7.4.0\navailable ram: 31.35 gb\navailable cpu: 4 core\n----\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Load the data simply\nX_train, y_train, X_test = crunch.load_data()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKHXgvjN-0fS","outputId":"3b7ef5e2-8db7-4d30-ff24-6acc894920b7","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:32:26.663032Z","iopub.execute_input":"2025-09-05T19:32:26.664070Z","iopub.status.idle":"2025-09-05T19:32:30.498022Z","shell.execute_reply.started":"2025-09-05T19:32:26.664007Z","shell.execute_reply":"2025-09-05T19:32:30.496860Z"}},"outputs":[{"name":"stdout","text":"data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\ndata/X_train.parquet: already exists, file length match\ndata/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\ndata/X_test.reduced.parquet: already exists, file length match\ndata/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\ndata/y_train.parquet: already exists, file length match\ndata/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\ndata/y_test.reduced.parquet: already exists, file length match\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T18:06:28.999954Z","iopub.execute_input":"2025-09-05T18:06:29.000369Z","iopub.status.idle":"2025-09-05T18:06:29.013087Z","shell.execute_reply.started":"2025-09-05T18:06:29.000342Z","shell.execute_reply":"2025-09-05T18:06:29.011558Z"}},"outputs":[{"name":"stdout","text":"(23715734, 2)\nstructural_breakpoint\nFalse    7092\nTrue     2909\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"Number of datasets:\", len(X_test))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ErbKAs--0fT","outputId":"5ce46294-1824-4067-be90-547189e90be3","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T18:06:47.924594Z","iopub.execute_input":"2025-09-05T18:06:47.924961Z","iopub.status.idle":"2025-09-05T18:06:47.931023Z","shell.execute_reply.started":"2025-09-05T18:06:47.924929Z","shell.execute_reply":"2025-09-05T18:06:47.930082Z"}},"outputs":[{"name":"stdout","text":"Number of datasets: 101\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### ðŸ”¥ train.py","metadata":{}},{"cell_type":"code","source":"%%writefile main.py\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import skew, kurtosis, ks_2samp, mannwhitneyu, wasserstein_distance\nfrom scipy.signal import welch, hilbert\nfrom statsmodels.tsa.stattools import acf, pacf\n# ======= Refactor-friendly train() and infer() =======\nimport os\nimport joblib\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.calibration import CalibratedClassifierCV\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# Suppress convergence warnings\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)  # suppress welch nperseg warnings\n\n# optional libs - safe to import if installed\ntry:\n    import antropy as ant\nexcept Exception:\n    ant = None\ntry:\n    import pywt\nexcept Exception:\n    pywt = None\n\n# --- helper funcs (robust + small fixes) ---\n# ---### ðŸ”¥ Advanced Feature Engineering Function-----\ndef compute_permutation_entropy(x, m=3, tau=1):\n    if ant is None or len(x) < (m + 1):\n        return np.nan\n    try:\n        return float(ant.perm_entropy(x, order=m, delay=tau, normalize=True))\n    except Exception:\n        return np.nan\n\ndef compute_multiscale_entropy(x, scales=(2,3,5)):\n    if ant is None:\n        return np.nan\n    vals = []\n    for s in scales:\n        xs = x[::s]\n        if len(xs) > 10:\n            try:\n                vals.append(float(ant.sample_entropy(xs)))\n            except Exception:\n                pass\n    return float(np.mean(vals)) if vals else np.nan\n\ndef spectral_features(x, fs=1.0):\n    # safe nperseg\n    nperseg = min(256, max(8, len(x)))\n    f, Pxx = welch(x, fs=fs, nperseg=nperseg)\n    # spectral entropy fallback\n    if ant is not None:\n        try:\n            se = float(ant.spectral_entropy(x, sf=fs, method='welch', normalize=True))\n        except Exception:\n            P = Pxx / (Pxx.sum() + 1e-12)\n            se = float(-np.sum(P * np.log(P + 1e-12)))\n    else:\n        P = Pxx / (Pxx.sum() + 1e-12)\n        se = float(-np.sum(P * np.log(P + 1e-12)))\n    centroid = float(np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12))\n    return se, centroid\n\ndef wavelet_energy(x, wavelet='db1', level=3):\n    if pywt is None:\n        return np.nan, np.nan\n    try:\n        coeffs = pywt.wavedec(x, wavelet, level=level)\n        energies = [np.sum(c**2) for c in coeffs if len(c) > 0]\n        return float(np.mean(energies)), float(np.std(energies))\n    except Exception:\n        return np.nan, np.nan\n\ndef lz_complexity(x):\n    if ant is None:\n        return np.nan\n    try:\n        sig = np.sign(x - np.nanmean(x)).astype(int)\n        return float(ant.lziv_complexity(sig))\n    except Exception:\n        return np.nan\n\ndef robust_stats(arr):\n    return {\n        'mean': float(np.nanmean(arr)),\n        'median': float(np.nanmedian(arr)),\n        'std': float(np.nanstd(arr)),\n        'iqr': float(np.nanpercentile(arr,75) - np.nanpercentile(arr,25))\n    }\n\n# --- cleaned main builder ---\ndef build_advanced_features(X_df, y_series):\n    \"\"\"\n    X_df: MultiIndex DataFrame (id, time) with columns ['value','period']\n    y_series: pd.Series indexed by id\n    Returns: X_features (DataFrame indexed by id), y_features (Series)\n    \"\"\"\n    rows = []\n    ids = []\n\n    for sid, df in X_df.groupby(level='id', sort=False):\n        df = df.dropna(subset=['value'])\n        before = df.loc[df['period']==0, 'value'].to_numpy(dtype=float)\n        after  = df.loc[df['period']==1, 'value'].to_numpy(dtype=float)\n\n        # skip too short segments\n        if len(before) < 8 or len(after) < 8:\n            continue\n\n        # basic stats once\n        b = robust_stats(before)\n        a = robust_stats(after)\n\n        feats = {\n            'b_mean': b['mean'], 'a_mean': a['mean'],\n            'b_median': b['median'], 'a_median': a['median'],\n            'b_std': b['std'], 'a_std': a['std'],\n            'b_iqr': b['iqr'], 'a_iqr': a['iqr'],\n        }\n\n        # diffs/ratios\n        feats['mean_diff'] = feats['a_mean'] - feats['b_mean']\n        feats['median_diff'] = feats['a_median'] - feats['b_median']\n        feats['std_diff'] = feats['a_std'] - feats['b_std']\n        feats['iqr_diff'] = feats['a_iqr'] - feats['b_iqr']\n        feats['std_ratio'] = (feats['a_std'] + 1e-9) / (feats['b_std'] + 1e-9)\n\n        # complexity: compute once per side (no duplicates)\n        feats['pe_m3_t1_diff'] = compute_permutation_entropy(after, m=3, tau=1) - compute_permutation_entropy(before, m=3, tau=1)\n        feats['pe_m5_t1_diff'] = compute_permutation_entropy(after, m=5, tau=1) - compute_permutation_entropy(before, m=5, tau=1)\n        feats['mse_diff'] = compute_multiscale_entropy(after) - compute_multiscale_entropy(before)\n\n        # distances/tests\n        try:\n            feats['emd'] = float(wasserstein_distance(before, after))\n        except:\n            feats['emd'] = np.nan\n        try:\n            feats['ks_stat'] = float(ks_2samp(before, after).statistic)\n        except:\n            feats['ks_stat'] = np.nan\n        try:\n            feats['mw_p'] = float(mannwhitneyu(before, after).pvalue)\n        except:\n            feats['mw_p'] = np.nan\n\n        # acf energy diff\n        try:\n            acf_b = np.nan_to_num(acf(before, nlags=20, fft=True))\n            acf_a = np.nan_to_num(acf(after, nlags=20, fft=True))\n            feats['acf_energy_diff'] = float(np.sum(acf_a**2) - np.sum(acf_b**2))\n        except:\n            feats['acf_energy_diff'] = np.nan\n\n        # spectral & wavelet\n        se_b, cent_b = spectral_features(before)\n        se_a, cent_a = spectral_features(after)\n        feats['spectral_entropy_diff'] = se_a - se_b\n        feats['spectral_centroid_diff'] = cent_a - cent_b\n\n        we_b_mean, we_b_std = wavelet_energy(before)\n        we_a_mean, we_a_std = wavelet_energy(after)\n        feats['wavelet_energy_diff'] = we_a_mean - we_b_mean\n\n        # hilbert instantaneous freq diff\n        try:\n            inst_b = np.mean(np.diff(np.unwrap(np.angle(hilbert(before)))))\n            inst_a = np.mean(np.diff(np.unwrap(np.angle(hilbert(after)))))\n            feats['hilbert_freq_diff'] = float(inst_a - inst_b)\n        except:\n            feats['hilbert_freq_diff'] = np.nan\n\n        # compression\n        feats['lz_diff'] = lz_complexity(after) - lz_complexity(before)\n\n        rows.append(feats)\n        ids.append(sid)\n\n    X_features = pd.DataFrame(rows, index=ids)\n    X_features.index.name = 'id'\n    X_features = X_features.replace([np.inf, -np.inf], np.nan).fillna(0)\n    y_features = y_series.loc[X_features.index].astype(int)\n\n    return X_features, y_features\n\n\ndef extract_features_series(df):\n    \"\"\"\n    df: DataFrame with columns ['value','period'] for a single id\n    returns: dict of features (same keys as build_advanced_features)\n    \"\"\"\n    df = df.dropna(subset=['value'])\n    before = df.loc[df['period']==0, 'value'].to_numpy(dtype=float)\n    after  = df.loc[df['period']==1, 'value'].to_numpy(dtype=float)\n\n    # skip too short segments\n    if len(before) < 8 or len(after) < 8:\n        return None\n\n    b = robust_stats(before)\n    a = robust_stats(after)\n\n    feats = {\n        'b_mean': b['mean'], 'a_mean': a['mean'],\n        'b_median': b['median'], 'a_median': a['median'],\n        'b_std': b['std'], 'a_std': a['std'],\n        'b_iqr': b['iqr'], 'a_iqr': a['iqr'],\n    }\n\n    feats['mean_diff']   = feats['a_mean'] - feats['b_mean']\n    feats['median_diff'] = feats['a_median'] - feats['b_median']\n    feats['std_diff']    = feats['a_std'] - feats['b_std']\n    feats['iqr_diff']    = feats['a_iqr'] - feats['b_iqr']\n    feats['std_ratio']   = (feats['a_std'] + 1e-9) / (feats['b_std'] + 1e-9)\n\n    # complexity\n    feats['pe_m3_t1_diff'] = compute_permutation_entropy(after, m=3, tau=1) - compute_permutation_entropy(before, m=3, tau=1)\n    feats['pe_m5_t1_diff'] = compute_permutation_entropy(after, m=5, tau=1) - compute_permutation_entropy(before, m=5, tau=1)\n    feats['mse_diff']      = compute_multiscale_entropy(after) - compute_multiscale_entropy(before)\n\n    # distances/tests\n    try:\n        feats['emd'] = float(wasserstein_distance(before, after))\n    except:\n        feats['emd'] = np.nan\n    try:\n        feats['ks_stat'] = float(ks_2samp(before, after).statistic)\n    except:\n        feats['ks_stat'] = np.nan\n    try:\n        feats['mw_p'] = float(mannwhitneyu(before, after).pvalue)\n    except:\n        feats['mw_p'] = np.nan\n\n    # acf energy diff\n    try:\n        acf_b = np.nan_to_num(acf(before, nlags=20, fft=True))\n        acf_a = np.nan_to_num(acf(after, nlags=20, fft=True))\n        feats['acf_energy_diff'] = float(np.sum(acf_a**2) - np.sum(acf_b**2))\n    except:\n        feats['acf_energy_diff'] = np.nan\n\n    # spectral & wavelet\n    se_b, cent_b = spectral_features(before)\n    se_a, cent_a = spectral_features(after)\n    feats['spectral_entropy_diff']  = se_a - se_b\n    feats['spectral_centroid_diff'] = cent_a - cent_b\n\n    we_b_mean, we_b_std = wavelet_energy(before)\n    we_a_mean, we_a_std = wavelet_energy(after)\n    feats['wavelet_energy_diff'] = we_a_mean - we_b_mean\n\n    # hilbert instantaneous freq diff\n    try:\n        inst_b = np.mean(np.diff(np.unwrap(np.angle(hilbert(before)))))\n        inst_a = np.mean(np.diff(np.unwrap(np.angle(hilbert(after)))))\n        feats['hilbert_freq_diff'] = float(inst_a - inst_b)\n    except:\n        feats['hilbert_freq_diff'] = np.nan\n\n    # compression\n    feats['lz_diff'] = lz_complexity(after) - lz_complexity(before)\n\n    return feats\n\n# assume build_advanced_features is in scope (the cleaned version we made)\n# assume get_base_models and train_and_validate exist or we will define inline.\n\ndef train(X_train: \"pd.DataFrame\", y_train: \"pd.Series\", model_directory_path: str):\n    \"\"\"\n    Crunch-style train function.\n    - X_train: MultiIndex DataFrame (id,time) with columns ['value','period']\n    - y_train: Series indexed by id\n    - model_directory_path: directory to save model.joblib\n    \"\"\"\n    os.makedirs(model_directory_path, exist_ok=True)\n    rng_seed = 42\n\n    # 1) Feature extraction (deterministic)\n    X_features, y_features = build_advanced_features(X_train, y_train)\n    print(f\"[train] Features built: {X_features.shape}, labels: {y_features.value_counts().to_dict()}\")\n\n    # 2) Ensure deterministic seeds in model definitions\n    # Build base models (MLP wrapped with scaler pipeline so scaler is saved inside)\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n    from sklearn.linear_model import LogisticRegression\n    import lightgbm as lgb\n    import xgboost as xgb\n    from sklearn.neural_network import MLPClassifier\n\n    neg = len(y_features) - int(y_features.sum())\n    pos = int(y_features.sum())\n    pos_ratio = neg / (pos + 1e-9)\n\n    base_models = [\n        (\"lgb\", lgb.LGBMClassifier(\n            n_estimators=400, learning_rate=0.05, num_leaves=31,\n            subsample=0.8, colsample_bytree=0.8, random_state=rng_seed,\n            class_weight=\"balanced\"\n        )),\n        (\"xgb\", xgb.XGBClassifier(\n            n_estimators=400, learning_rate=0.05, max_depth=5,\n            subsample=0.8, colsample_bytree=0.8,\n            eval_metric=\"logloss\", use_label_encoder=False,\n            random_state=rng_seed, scale_pos_weight=pos_ratio\n        )),\n        (\"rf\", RandomForestClassifier(\n            n_estimators=300, max_depth=10, n_jobs=-1, random_state=rng_seed,\n            class_weight=\"balanced\"\n        )),\n        (\"mlp\", Pipeline([\n            (\"scaler\", StandardScaler()),\n            (\"mlp\", MLPClassifier(hidden_layer_sizes=(64,), max_iter=300,\n                                  random_state=rng_seed))\n        ])),\n        # you may add a logistic wrapper if you like\n    ]\n\n    # 3) Per-base-model CV to compute OOF preds (GroupKFold)\n    gkf = GroupKFold(n_splits=5)\n    # store OOF predictions for stacking diagnostics\n    oof_preds_per_model = {name: np.zeros(len(X_features)) for name, _ in base_models}\n    X_idx = np.arange(len(X_features))\n    groups = X_features.index\n\n    for name, model in base_models:\n        print(f\"[train] CV for base model: {name}\")\n        fold_preds = np.zeros(len(X_features))\n        for fold, (tr_idx, val_idx) in enumerate(gkf.split(X_features, y_features, groups)):\n            X_tr, X_val = X_features.iloc[tr_idx], X_features.iloc[val_idx]\n            y_tr, y_val = y_features.iloc[tr_idx], y_features.iloc[val_idx]\n\n            # fit\n            model.fit(X_tr, y_tr)\n            proba = model.predict_proba(X_val)[:, 1]\n            fold_preds[val_idx] = proba\n            print(f\"   fold {fold} | {name} ROC-AUC: {roc_auc_score(y_val, proba):.4f}\")\n\n        oof_score = roc_auc_score(y_features, fold_preds)\n        print(f\" [train] {name} OOF ROC-AUC: {oof_score:.4f}\")\n        oof_preds_per_model[name] = fold_preds.copy()\n\n    # 4) Fit calibrated base models on full data for stacking (optional calibration)\n    calibrated_estimators = []\n    for name, model in base_models:\n        print(f\"[train] Fitting & calibrating full model: {name}\")\n        model.fit(X_features, y_features)\n        calibrated = CalibratedClassifierCV(model, method=\"isotonic\", cv=3)\n        # CalibratedClassifierCV.fit will refit internally; this can be slow but yields calibrated probs.\n        calibrated.fit(X_features, y_features)\n        calibrated_estimators.append((name, calibrated))\n\n    # 5) Meta learner: LightGBM (non-linear)\n    meta = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=15,\n                              subsample=0.8, colsample_bytree=0.8, random_state=rng_seed,\n                              class_weight=\"balanced\")\n\n    # 6) Build stacking classifier with passthrough (so original features are also available)\n    stack = StackingClassifier(\n        estimators=calibrated_estimators,\n        final_estimator=meta,\n        stack_method=\"predict_proba\",\n        passthrough=True,\n        cv=5,  # internal cv inside sklearn stacking for safety\n        n_jobs=-1\n    )\n\n    # Fit final stacking on full training features\n    stack.fit(X_features, y_features)\n    final_oof = roc_auc_score(y_features, stack.predict_proba(X_features)[:, 1])\n    print(f\"[train] Final stacking OOF ROC-AUC (on train features): {final_oof:.4f}\")\n\n    # 7) Save artifact: include feature columns and pipeline\n    artifact = {\n        \"feature_cols\": list(X_features.columns),\n        \"stacking_model\": stack,\n        \"oof_per_model\": oof_preds_per_model,\n        \"train_oof_score\": float(final_oof),\n        \"random_seed\": rng_seed\n    }\n    # âœ… Aur ab ye do lines daalo:\n    os.makedirs(model_directory_path, exist_ok=True)\n    joblib.dump(artifact, os.path.join(model_directory_path, \"model.joblib\"))\n    print(\"[train] Saved artifact to\", os.path.join(model_directory_path, \"model.joblib\"))\n    return\n\ndef infer(X_test, model_directory_path):\n    \"\"\"\n    Crunch-style infer generator. Yield once, then yield probabilities for each test series (one df at a time).\n    X_test: iterable of pd.DataFrame (single-series DataFrames) as Crunch expects.\n    \"\"\"\n    import os, joblib, pandas as pd\n\n    artifact = joblib.load(os.path.join(model_directory_path, \"model.joblib\"))\n    feature_cols = artifact[\"feature_cols\"]\n    stack = artifact[\"stacking_model\"]\n\n    # signal ready\n    yield\n\n    if X_test is None:\n        # safety check\n        return\n\n    # process each test DF (single series) and extract features then predict\n    for df in X_test:\n        feats = extract_features_series(df)\n        if feats is None:\n            yield 0.0\n            continue\n        x = pd.DataFrame([feats])\n        x = x.reindex(columns=feature_cols).fillna(0)\n        proba = float(stack.predict_proba(x)[0, 1])\n        yield proba\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:10:14.585548Z","iopub.execute_input":"2025-09-05T23:10:14.586021Z","iopub.status.idle":"2025-09-05T23:10:14.602723Z","shell.execute_reply.started":"2025-09-05T23:10:14.585993Z","shell.execute_reply":"2025-09-05T23:10:14.601629Z"}},"outputs":[{"name":"stdout","text":"Overwriting main.py\n[LightGBM] [Info] Number of positive: 1939, number of negative: 4728\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022435 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6057\n[LightGBM] [Info] Number of data points in the train set: 6667, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Info] Number of positive: 1939, number of negative: 4728\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002052 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6054\n[LightGBM] [Info] Number of data points in the train set: 6667, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Info] Number of positive: 1940, number of negative: 4728\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000920 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 6058\n[LightGBM] [Info] Number of data points in the train set: 6668, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n","output_type":"stream"}],"execution_count":96},{"cell_type":"markdown","source":"## Local testing\n\nTo make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\nEven if it is not perfect, it should give you a quick idea if your model is working properly.","metadata":{"id":"1W0Kl9CA-0fU"}},{"cell_type":"code","source":"crunch.test(\n    # Uncomment to disable the train\n    #force_first_train=False,\n\n    # Uncomment to disable the determinism check\n    # no_determinism_check=True,\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDZeP-4--0fU","outputId":"8bfdb1e2-446e-4cfe-9888-c9f07fb4aeab","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T23:10:22.441740Z","iopub.execute_input":"2025-09-05T23:10:22.442099Z"}},"outputs":[{"name":"stderr","text":"23:10:22 forbidden library: feature_builder  (request to whitelist: https://hub.crunchdao.com/competitions/structural-break/resources/whitelisted-libraries?requestAlias=feature_builder)\n23:10:22 \n23:10:22 started\n23:10:22 running local test\n23:10:22 internet access isn't restricted, no check will be done\n23:10:22 \n23:10:23 starting unstructured loop...\n23:10:23 executing - command=train\n","output_type":"stream"},{"name":"stdout","text":"data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\ndata/X_train.parquet: already exists, file length match\ndata/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\ndata/X_test.reduced.parquet: already exists, file length match\ndata/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\ndata/y_train.parquet: already exists, file length match\ndata/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\ndata/y_test.reduced.parquet: already exists, file length match\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Results\n\nOnce the local tester is done, you can preview the result stored in `data/prediction.parquet`.","metadata":{"id":"bV_5CKs--0fU"}},{"cell_type":"code","source":"prediction = pd.read_parquet(\"data/prediction.parquet\")\nprediction","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"ly5q68sA-0fU","outputId":"4ab7c035-6bdc-41dc-85b5-c11c8ddc6d1a","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T21:17:06.378024Z","iopub.execute_input":"2025-09-05T21:17:06.381182Z","iopub.status.idle":"2025-09-05T21:17:06.465750Z","shell.execute_reply.started":"2025-09-05T21:17:06.381126Z","shell.execute_reply":"2025-09-05T21:17:06.463753Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3034219019.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/prediction.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/prediction.parquet'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data/prediction.parquet'","output_type":"error"}],"execution_count":80},{"cell_type":"markdown","source":"### Local scoring\n\nYou can call the function that the system uses to estimate your score locally.","metadata":{"id":"1oP-NLGh-0fU"}},{"cell_type":"code","source":"# Load the targets\ntarget = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n\n# Call the scoring function\nsklearn.metrics.roc_auc_score(\n    target,\n    prediction,\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyCrjpzv-0fU","outputId":"4dd5e952-db2d-465f-b231-7bf606161b19","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T22:24:33.216475Z","iopub.execute_input":"2025-09-05T22:24:33.216880Z","iopub.status.idle":"2025-09-05T22:24:33.252861Z","shell.execute_reply.started":"2025-09-05T22:24:33.216855Z","shell.execute_reply":"2025-09-05T22:24:33.249863Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1678923823.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m sklearn.metrics.roc_auc_score(\n\u001b[1;32m      6\u001b[0m     \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"],"ename":"NameError","evalue":"name 'prediction' is not defined","output_type":"error"}],"execution_count":86},{"cell_type":"markdown","source":"# Submit your Notebook\n\nTo submit your work, you must:\n1. Download your Notebook from Kaggle\n2. Upload it to the platform\n3. Create a run to validate it\n\n### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n\n![Download and Submit Notebook](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/download-and-submit-notebook-on-kaggle.gif)","metadata":{"id":"3AE1i3pR-0fV"}}]}