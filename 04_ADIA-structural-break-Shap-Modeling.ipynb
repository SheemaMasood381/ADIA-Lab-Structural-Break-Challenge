{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìå ADIA Lab Structural Break Challenge  \n\n**Assalam o Alaikumüëã**  \nIn this notebook, we are going to explore the concept of **structural breaks (regime shifts)** ‚Äì basically jab data ka trend, mean, or variance suddenly change ho jaye üìä.  \n\nStructural break detection is an important problem because real-world data kabhi bhi smooth aur stable nahi hota. Kabhi kabhi beech me major shifts aate hain jo forecasting aur analysis dono ko effect kar dete hain.  \n\n---\n\n## üîé Challenge Overview  \n\nWelcome to the **ADIA Lab Structural Break Challenge!**  \nIn this competition, you will analyze **univariate time series data** to determine whether a **structural break** has occurred at a specified boundary point.  \n\n### üìñ What is a Structural Break?  \n\nA **structural break** occurs when the process governing the data generation changes at a certain point in time.  \nThese changes can be subtle or dramatic, and detecting them accurately is crucial across domains:  \n\n- üå¶ **Climatology** ‚Üí shifts in long-term weather patterns  \n- üè≠ **Industrial Monitoring** ‚Üí detecting sudden machine behavior changes  \n- üíπ **Finance** ‚Üí market crashes or regime shifts  \n- üè• **Healthcare** ‚Üí sudden change in patient health indicators  \n\n![Structural Break Example](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/structural-break/quickstarters/baseline/images/example.png)  \n\n---\n\n## üìù Our Task  \n\nFor each time series in the **test set**, we need to predict a **score between `0` and `1`:**  \n\n- `0` ‚Üí No structural break at the specified boundary point  \n- `1` ‚Üí A structural break **did occur**  \n\n---\n\n## üìä Evaluation Metric  \n\nThe challenge uses **ROC AUC (Area Under the Receiver Operating Characteristic Curve)** as the evaluation metric:  \n\n- **ROC AUC ‚âà 0.5** ‚Üí No better than random guessing  \n- **ROC AUC ‚Üí 1.0** ‚Üí Perfect detection performance  \n\nMore about ROC AUC: [sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)  \n\n---\n\n## üìÇ Notebook Flow üöÄ  \n\n1. **Exploratory Data Analysis (EDA)** ‚Äì visualize aur samajhenge dataset.  \n2. **Methods** ‚Äì different techniques (statistical + ML-based) try karenge for break detection.  \n3. **Evaluation** ‚Äì compare karenge results aur dekhenge kaun sa method best perform karta hai.  \n\n---\n\n‚ö° **Goal**: A clean, reproducible, and easy-to-follow Kaggle-style notebook ‚Äì jahan beginner bhi seekh le aur advanced banda bhi enjoy kare.  \n\n**Chalo shuru karte hain üöÄ**  \n","metadata":{}},{"cell_type":"code","source":"# import Important Libraries\n\n!pip install antropy --quiet\n!pip install PyWavelets --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:29:52.604668Z","iopub.execute_input":"2025-09-08T20:29:52.604962Z","iopub.status.idle":"2025-09-08T20:30:01.865596Z","shell.execute_reply.started":"2025-09-08T20:29:52.604939Z","shell.execute_reply":"2025-09-08T20:30:01.864396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport typing\n\n# Import your dependencies\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport sklearn.metrics\nfrom scipy.stats import wasserstein_distance  # 1D Earth Mover's Distance\nfrom scipy.stats import skew, kurtosis, ks_2samp\nfrom scipy.stats import wasserstein_distance\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report, RocCurveDisplay\nfrom scipy.stats import skew, kurtosis, ks_2samp, mannwhitneyu, wasserstein_distance\nfrom scipy.signal import welch, hilbert\nfrom statsmodels.tsa.stattools import acf, pacf\n\nfrom scipy.signal import welch\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# Ignore UserWarning\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Ignore ConvergenceWarning\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:31:04.200098Z","iopub.execute_input":"2025-09-08T20:31:04.200807Z","iopub.status.idle":"2025-09-08T20:31:04.209002Z","shell.execute_reply.started":"2025-09-08T20:31:04.200775Z","shell.execute_reply":"2025-09-08T20:31:04.207921Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Crunch CLI SETUP","metadata":{}},{"cell_type":"code","source":"### CRunch CLI Setup\n%pip install crunch-cli --upgrade --quiet --progress-bar off\n!crunch setup-notebook structural-break xm5KtLMSrYthCM1fOS5CsfpQ\n\n\nimport crunch\n\n# Load the Crunch Toolings\ncrunch = crunch.load_notebook()\n\n# Load the data simply\nX_train, y_train, X_test = crunch.load_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:31:38.157258Z","iopub.execute_input":"2025-09-08T20:31:38.157602Z","iopub.status.idle":"2025-09-08T20:31:56.471822Z","shell.execute_reply.started":"2025-09-08T20:31:38.157573Z","shell.execute_reply":"2025-09-08T20:31:56.470559Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### BASIC EDA","metadata":{}},{"cell_type":"code","source":"# --- Dataset Overview ---\nprint(\"üìÇ Total training subsequences:\", len(X_train))\nprint(X_train.shape)\nprint(\"üìÇ Total test subsequences:\", len(X_test))\nprint(\"üìÇ y_train length:\", len(y_train))\n\n# --- Target Labels ---\nprint(\"\\nüìä y_train distribution:\")\nprint(y_train.value_counts(normalize=True))\nprint(y_train.value_counts())\n\n# --- Check Nulls in y_train ---\nprint(\"\\nüìâ Missing values in y_train:\", y_train.isnull().sum())\n\n# --- Test set EDA (corrected) ---\nprint(\"\\nüìÇ Test set overview:\")\nprint(\"Number of test series:\", len(X_test))\nprint(\"Shape of first 5 test series:\", [np.array(df).shape for df in X_test[:5]])\n\n# --- Nulls in X_train (safe) ---\ntrain_nulls = 0\nfor x in X_train[:1000]:  # sample 1000 series\n    try:\n        arr = np.array(x, dtype=float)\n        train_nulls += np.isnan(arr).sum()\n    except:\n        # non-numeric series\n        train_nulls += 0\n\nprint(f\"\\nüìâ Null values in first 1000 training series: {train_nulls}\")\n\n# --- Nulls in X_test (safe) ---\ntest_nulls = 0\nfor x in X_test:\n    try:\n        arr = np.array(x, dtype=float)\n        test_nulls += np.isnan(arr).sum()\n    except:\n        test_nulls += 0\n\nprint(f\"üìâ Null values in test series: {test_nulls}\")\n\n# --- Example lengths of sequences ---\ntrain_lengths = [len(x) for x in X_train[:1000]]  # sample\nprint(f\"\\nüìè Training series length stats (sample 1000):\")\nprint(\"Number of datasets:\", len(X_test))\nprint(pd.Series(train_lengths).describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:31:57.058922Z","iopub.execute_input":"2025-09-08T20:31:57.059273Z","iopub.status.idle":"2025-09-08T20:31:57.099898Z","shell.execute_reply.started":"2025-09-08T20:31:57.059243Z","shell.execute_reply":"2025-09-08T20:31:57.098602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìù Basic EDA Notes (Updated)\n\n- **Total Training Subsequences:** 23,715,734  \n- **Total Test Subsequences:** 101  \n- **y_train Length:** 10,001  \n\n### üìä y_train Distribution\n- **Proportion:**  \n  - False: 70.9%  \n  - True: 29.1%  \n- **Counts:**  \n  - False: 7,092  \n  - True: 2,909  \n- Label imbalance exists ‚Üí metric ROC AUC is robust to this.\n\n### üìâ Missing Values\n- **y_train:** 0  \n- **X_train (first 1000 series):** 0  \n- **X_test:** 0  \n- No missing data ‚Üí preprocessing easier.\n\n### üìÇ Test Set Overview\n- Number of test series: 101  \n- Shape of first 5 test series: `(2779,2), (1966,2), (1775,2), (3296,2), (1995,2)`  \n- Each test series has variable length but 2 columns (probably period & value).\n\n### üìè Training Series Lengths (Sample 1000)\n- Min: 5, Max: 6, Mean: 5.5  \n- Mostly very short sequences ‚Üí statistical/ML models will work on small windows.\n\n**Notes:**  \n- Dataset has **very large number of training subsequences**  \n- No missing values ‚Üí safe to use directly  \n- Label imbalance is moderate ‚Üí consider for evaluation metric  \n- Next steps: visualize sample series, inspect True vs False structural breaks.\n","metadata":{}},{"cell_type":"markdown","source":"### Visualization of series","metadata":{}},{"cell_type":"code","source":"# visualize a single time series\ndef plot_series(series_id):\n    df = X_train.loc[series_id]\n    plt.figure(figsize=(12,4))\n    plt.plot(df.index, df[\"value\"], label=\"value\")\n    plt.axvline(x=df[df[\"period\"]==0].index.max(), color=\"red\", linestyle=\"--\", label=\"boundary\")\n    plt.title(f\"Series ID {series_id} | Label: {y_train.loc[series_id]}\")\n    plt.legend()\n    plt.show()\n\nplot_series(0)\nplot_series(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:11:21.831363Z","iopub.execute_input":"2025-09-08T20:11:21.831675Z","iopub.status.idle":"2025-09-08T20:11:22.599925Z","shell.execute_reply.started":"2025-09-08T20:11:21.831650Z","shell.execute_reply":"2025-09-08T20:11:22.598800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Length of series","metadata":{}},{"cell_type":"code","source":"#Length of series\nseries_lengths = X_train.groupby(\"id\").size()\ndisplay(series_lengths.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:11:26.409010Z","iopub.execute_input":"2025-09-08T20:11:26.409516Z","iopub.status.idle":"2025-09-08T20:11:27.066661Z","shell.execute_reply.started":"2025-09-08T20:11:26.409479Z","shell.execute_reply":"2025-09-08T20:11:27.065498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Mean aur Std deviation comparison before and after the structural break:\n","metadata":{}},{"cell_type":"code","source":"# Get indices\ntrue_idx = y_train[y_train==True].index[0]\nfalse_idx = y_train[y_train==False].index[0]\n\ndef compare_stats_plot(series_id):\n    df = X_train.loc[series_id]  # assuming period & value columns\n    before = df[df[\"period\"]==0][\"value\"]\n    after  = df[df[\"period\"]==1][\"value\"]\n    \n    print(f\"Series {series_id} | Label: {y_train.loc[series_id]}\")\n    print(\"Before mean:\", before.mean(), \" | std:\", before.std())\n    print(\"After  mean:\", after.mean(), \" | std:\", after.std())\n    \n    # Plot\n    plt.figure(figsize=(10,4))\n    plt.plot(before.values, marker='o', label='Before Break')\n    plt.plot(after.values, marker='o', label='After Break')\n    plt.title(f\"Series {series_id} | Label: {y_train.loc[series_id]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Value\")\n    plt.legend()\n    plt.show()\n\n# Compare True series\ncompare_stats_plot(true_idx)\n\n# Compare False series\ncompare_stats_plot(false_idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:11:30.823324Z","iopub.execute_input":"2025-09-08T20:11:30.823663Z","iopub.status.idle":"2025-09-08T20:11:31.373117Z","shell.execute_reply.started":"2025-09-08T20:11:30.823641Z","shell.execute_reply":"2025-09-08T20:11:31.371978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîç Structural Break Interpretation\n\n**Series 0 | Label: False**  \n- Mean before: 1.46e-5 | after: 6.37e-6  \n- Std before: 0.00699 | after: 0.00688  \n- Observation: Mean and std are almost the same ‚Üí **No significant change**  \n- Conclusion: **No structural break**  \n\n**Series 2 | Label: True**  \n- Mean before: 0.000389 | after: 0.00179  \n- Std before: 0.0172 | after: 0.0229  \n- Observation: Both mean and std increased ‚Üí **Noticeable change in distribution**  \n- Conclusion: **Structural break exists**  \n\n**Summary:**  \n- **False series:** stable values, no break  \n- **True series:** sudden change in mean/variance ‚Üí structural break\n","metadata":{}},{"cell_type":"markdown","source":"### Compariseon true and fase labels","metadata":{}},{"cell_type":"code","source":"# Compariseon true and fase labels\ndef compare_true_false(X_train, y_train, true_id=None, false_id=None):\n    # agar ID pass na ki ho to first True/False pick karo\n    if true_id is None:\n        true_id = y_train[y_train == True].index[0]\n    if false_id is None:\n        false_id = y_train[y_train == False].index[0]\n    \n    for series_id in [true_id, false_id]:\n        df = X_train.loc[series_id]\n        before = df[df[\"period\"]==0][\"value\"]\n        after  = df[df[\"period\"]==1][\"value\"]\n        \n        fig, axes = plt.subplots(1, 2, figsize=(14,4))\n        \n        # time series plot\n        axes[0].plot(df.index, df[\"value\"], alpha=0.7)\n        boundary = df[df[\"period\"]==0].index.max()\n        axes[0].axvline(boundary, color=\"red\", linestyle=\"--\", label=\"boundary\")\n        axes[0].set_title(f\"Series {series_id} | Label: {y_train.loc[series_id]}\")\n        axes[0].legend()\n        \n        # histogram compare\n        axes[1].hist(before, bins=50, alpha=0.5, label=\"before\")\n        axes[1].hist(after, bins=50, alpha=0.5, label=\"after\")\n        axes[1].set_title(\"Distribution before vs after\")\n        axes[1].legend()\n        \n        plt.suptitle(f\"Stats ‚Üí Before mean={before.mean():.4f}, std={before.std():.4f} | After mean={after.mean():.4f}, std={after.std():.4f}\", fontsize=10)\n        plt.show()\n\n# run example\ncompare_true_false(X_train, y_train)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:11:38.562827Z","iopub.execute_input":"2025-09-08T20:11:38.563759Z","iopub.status.idle":"2025-09-08T20:11:39.739409Z","shell.execute_reply.started":"2025-09-08T20:11:38.563726Z","shell.execute_reply":"2025-09-08T20:11:39.738161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T20:11:43.293751Z","iopub.execute_input":"2025-09-08T20:11:43.294130Z","iopub.status.idle":"2025-09-08T20:11:43.322261Z","shell.execute_reply.started":"2025-09-08T20:11:43.294105Z","shell.execute_reply":"2025-09-08T20:11:43.321344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîπ Advanced Feature Engineering Strategy\n\nOur goal is to transform raw time series data into **informative features** that capture both statistical properties and structural changes between \"before\" and \"after\" segments. This enables machine learning models to detect subtle patterns indicative of structural breaks or anomalies.\n\n---\n\n### 1Ô∏è‚É£ Segmentation\n- Each time series is divided into two segments based on the `period` column:\n  - **Before (period=0)**\n  - **After  (period=1)**\n- Null values are dropped to ensure robust calculations.\n- Short segments (<8 samples) are skipped to maintain reliability.\n\n---\n\n### 2Ô∏è‚É£ Basic Statistical Features\nFor both segments, compute:\n- **Mean & Median** ‚Üí central tendency\n- **Standard Deviation (std)** ‚Üí spread\n- **Interquartile Range (IQR)** ‚Üí variability\n\n**Differences & Ratios:**\n- Mean difference (`mean_diff = after_mean - before_mean`)\n- Median difference (`median_diff`)\n- Std difference & ratio (`std_diff`, `std_ratio`)\n- IQR difference (`iqr_diff`)\n\nThese capture shifts in distribution after the structural break.\n\n---\n\n### 3Ô∏è‚É£ Complexity & Entropy Features\n- **Permutation Entropy (PE)** ‚Üí measure randomness/complexity of the signal\n- **Multiscale Sample Entropy (MSE)** ‚Üí complexity over multiple scales\n- Compute differences between `after` and `before`:\n  - `pe_m3_t1_diff`, `pe_m5_t1_diff`, `mse_diff`\n\n---\n\n### 4Ô∏è‚É£ Distance & Statistical Tests\n- **Wasserstein Distance (EMD)** ‚Üí distributional shift\n- **Kolmogorov-Smirnov Statistic (KS)** ‚Üí statistical difference between distributions\n- **Mann-Whitney U Test p-value** ‚Üí non-parametric comparison\n\n---\n\n### 5Ô∏è‚É£ Autocorrelation & Frequency Features\n- **ACF Energy Difference** ‚Üí captures change in temporal correlation\n- **Spectral Features**:\n  - Spectral Entropy difference ‚Üí disorder in frequency domain\n  - Spectral Centroid difference ‚Üí shift in frequency ‚Äúcenter of mass‚Äù\n- **Wavelet Energy Difference** ‚Üí change in localized energy across scales\n- **Hilbert Instantaneous Frequency Difference** ‚Üí change in dominant instantaneous frequency\n\n---\n\n### 6Ô∏è‚É£ Compression & Complexity\n- **Lempel-Ziv Complexity (LZ)** difference ‚Üí change in signal compressibility\n\n---\n\n### 7Ô∏è‚É£ Robustness\n- All features handle missing values and small sequences safely.\n- Infinite values replaced with 0.\n- Optional libraries like `antropy` and `pywt` used if available, with safe fallbacks.\n\n---\n\n### ‚úÖ Key Insight\n- This approach captures **both statistical and structural changes** in time series.\n- Combines **distributional, spectral, temporal, and complexity-based features**.\n- Creates a rich feature matrix suitable for **machine learning models** like Logistic Regression, Random Forest, or XGBoost.\n\n","metadata":{}},{"cell_type":"markdown","source":"### üî• Feature Pipeline Structure (According to SHAP and correlation analysis ) in previous notebook","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew, kurtosis, ks_2samp, wasserstein_distance, linregress\nfrom scipy.signal import hilbert, welch\nfrom statsmodels.tsa.stattools import acf\nimport numpy as np\nimport pandas as pd\nimport antropy as ant\nimport pywt\n\n# ---- Helper Functions ----\ndef robust_stats(arr):\n    return {\n        'mean': float(np.nanmean(arr)),\n        'median': float(np.nanmedian(arr)),\n        'std': float(np.nanstd(arr)),\n        'iqr': float(np.nanpercentile(arr, 75) - np.nanpercentile(arr, 25))\n    }\n\ndef slope_feature(x):\n    try:\n        slope, _, _, _, _ = linregress(range(len(x)), x)\n        return float(slope)\n    except:\n        return np.nan\n\ndef fft_features(x):\n    try:\n        fft_vals = np.abs(np.fft.rfft(x))\n        if len(fft_vals) == 0:\n            return np.nan, np.nan, np.nan\n        return float(np.max(fft_vals)), float(np.mean(fft_vals)), int(np.argmax(fft_vals))\n    except:\n        return np.nan, np.nan, np.nan\n\ndef spectral_features(x, fs=1.0):\n    nperseg = min(256, max(8, len(x)))\n    f, Pxx = welch(x, fs=fs, nperseg=nperseg)\n    try:\n        se = float(ant.spectral_entropy(x, sf=fs, method='welch', normalize=True))\n    except:\n        P = Pxx / (Pxx.sum() + 1e-12)\n        se = float(-np.sum(P * np.log(P + 1e-12)))\n    centroid = float(np.sum(f * Pxx) / (np.sum(Pxx) + 1e-12))\n    return se, centroid\n\ndef hilbert_features(x):\n    try:\n        analytic = hilbert(x)\n        amp_env = np.abs(analytic)\n        return float(np.mean(amp_env)), float(np.std(amp_env))\n    except:\n        return np.nan, np.nan\n\ndef compute_permutation_entropy(x, m=3, tau=1):\n    try:\n        return float(ant.perm_entropy(x, order=m, delay=tau, normalize=True))\n    except:\n        return np.nan\n\ndef compute_multiscale_entropy(x, scales=(2,3,5)):\n    vals = []\n    for s in scales:\n        xs = x[::s]\n        if len(xs) > 10:\n            try:\n                vals.append(float(ant.sample_entropy(xs)))\n            except:\n                pass\n    return float(np.mean(vals)) if vals else np.nan\n\ndef lz_complexity(x):\n    try:\n        sig = np.sign(x - np.nanmean(x)).astype(int)\n        return float(ant.lziv_complexity(sig))\n    except:\n        return np.nan\n\n# ---- Main Feature Builder ----# -\n\ndef build_selected_features(X_df, y_series):\n    rows, ids = [], []\n\n    for sid, df in X_df.groupby(level='id', sort=False):\n        df = df.dropna(subset=['value'])\n        before = df.loc[df['period']==0, 'value'].to_numpy(dtype=float)\n        after  = df.loc[df['period']==1, 'value'].to_numpy(dtype=float)\n\n        if len(before) < 8 or len(after) < 8:\n            continue\n\n        b = robust_stats(before)\n        a = robust_stats(after)\n\n        feats = {\n            # basic stats\n            'b_mean': b['mean'], 'a_mean': a['mean'],\n            'b_median': b['median'], 'a_median': a['median'],\n            'b_std': b['std'], 'a_std': a['std'],\n            'b_iqr': b['iqr'], 'a_iqr': a['iqr'],\n\n            # diffs/ratios\n            'mean_diff': a['mean'] - b['mean'],\n            'median_diff': a['median'] - b['median'],\n            'iqr_diff': a['iqr'] - b['iqr'],\n            'std_ratio': (a['std']+1e-9)/(b['std']+1e-9),\n        }\n\n        # shape\n        feats['skew_diff']  = skew(after) - skew(before)\n        feats['kurt_diff']  = kurtosis(after) - kurtosis(before)\n\n        # slope/trend\n        feats['slope_diff'] = slope_feature(after) - slope_feature(before)\n\n        # fft/spectral\n        max_b, _, peak_b = fft_features(before)\n        max_a, _, peak_a = fft_features(after)\n        feats['fft_peak_diff'] = max_a - max_b\n        feats['fft_domfreq_shift'] = peak_a - peak_b\n\n        se_b, cent_b = spectral_features(before)\n        se_a, cent_a = spectral_features(after)\n        feats['spectral_entropy_diff'] = se_a - se_b\n        feats['spectral_centroid_diff'] = cent_a - cent_b\n\n        # acf\n        for lag in [1,2,5,10]:\n            try:\n                feats[f'acf_lag{lag}_diff'] = acf(after, nlags=lag, fft=True)[lag] - acf(before, nlags=lag, fft=True)[lag]\n            except:\n                feats[f'acf_lag{lag}_diff'] = np.nan\n\n        # entropy family\n        feats['perm_entropy_diff'] = compute_permutation_entropy(after) - compute_permutation_entropy(before)\n        feats['ms_entropy_diff']   = compute_multiscale_entropy(after) - compute_multiscale_entropy(before)\n        feats['lz_complexity_diff'] = lz_complexity(after) - lz_complexity(before)\n\n        # distances/tests\n        try:\n            feats['ks_stat'] = ks_2samp(before, after).statistic\n        except:\n            feats['ks_stat'] = np.nan\n        try:\n            feats['wasserstein'] = wasserstein_distance(before, after)\n        except:\n            feats['wasserstein'] = np.nan\n\n        # hilbert mean only\n        hb_mean_b, _ = hilbert_features(before)\n        hb_mean_a, _ = hilbert_features(after)\n        feats['hilbert_mean_diff'] = hb_mean_a - hb_mean_b\n\n        rows.append(feats)\n        ids.append(sid)\n\n    X_features = pd.DataFrame(rows, index=ids)\n    X_features.index.name = 'id'\n    X_features = X_features.replace([np.inf, -np.inf], np.nan).fillna(0)\n    y_features = y_series.loc[X_features.index].astype(int)\n    return X_features, y_features\n\n\n# ---- Single-series extractor ----\ndef extract_seleted_features_series(df):\n    df = df.dropna(subset=['value'])\n    before = df.loc[df['period']==0, 'value'].to_numpy(dtype=float)\n    after  = df.loc[df['period']==1, 'value'].to_numpy(dtype=float)\n\n    if len(before) < 8 or len(after) < 8:\n        return None\n\n    b = robust_stats(before)\n    a = robust_stats(after)\n\n    feats = {\n        # basic stats\n        'b_mean': b['mean'], 'a_mean': a['mean'],\n        'b_median': b['median'], 'a_median': a['median'],\n        'b_std': b['std'], 'a_std': a['std'],\n        'b_iqr': b['iqr'], 'a_iqr': a['iqr'],\n\n        # diffs/ratios\n        'mean_diff': a['mean'] - b['mean'],\n        'median_diff': a['median'] - b['median'],\n        'iqr_diff': a['iqr'] - b['iqr'],\n        'std_ratio': (a['std']+1e-9)/(b['std']+1e-9),\n    }\n\n    # shape\n    feats['skew_diff']  = skew(after) - skew(before)\n    feats['kurt_diff']  = kurtosis(after) - kurtosis(before)\n\n    # slope/trend\n    feats['slope_diff'] = slope_feature(after) - slope_feature(before)\n\n    # fft/spectral\n    max_b, _, peak_b = fft_features(before)\n    max_a, _, peak_a = fft_features(after)\n    feats['fft_peak_diff'] = max_a - max_b\n    feats['fft_domfreq_shift'] = peak_a - peak_b\n\n    se_b, cent_b = spectral_features(before)\n    se_a, cent_a = spectral_features(after)\n    feats['spectral_entropy_diff'] = se_a - se_b\n    feats['spectral_centroid_diff'] = cent_a - cent_b\n\n    # acf\n    for lag in [1,2,5,10]:\n        try:\n            feats[f'acf_lag{lag}_diff'] = acf(after, nlags=lag, fft=True)[lag] - acf(before, nlags=lag, fft=True)[lag]\n        except:\n            feats[f'acf_lag{lag}_diff'] = np.nan\n\n    # entropy family\n    feats['perm_entropy_diff'] = compute_permutation_entropy(after) - compute_permutation_entropy(before)\n    feats['ms_entropy_diff']   = compute_multiscale_entropy(after) - compute_multiscale_entropy(before)\n    feats['lz_complexity_diff'] = lz_complexity(after) - lz_complexity(before)\n\n    # distances/tests\n    try:\n        feats['ks_stat'] = ks_2samp(before, after).statistic\n    except:\n        feats['ks_stat'] = np.nan\n    try:\n        feats['wasserstein'] = wasserstein_distance(before, after)\n    except:\n        feats['wasserstein'] = np.nan\n\n    # hilbert mean only\n    hb_mean_b, _ = hilbert_features(before)\n    hb_mean_a, _ = hilbert_features(after)\n    feats['hilbert_mean_diff'] = hb_mean_a - hb_mean_b\n\n    return feats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T21:40:43.530975Z","iopub.execute_input":"2025-09-08T21:40:43.531312Z","iopub.status.idle":"2025-09-08T21:40:43.563493Z","shell.execute_reply.started":"2025-09-08T21:40:43.531287Z","shell.execute_reply":"2025-09-08T21:40:43.562415Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# --- Apply feature engineering to check the impact ---\nX_features, y_features = build_selected_features(X_train, y_train)\nprint(X_features.shape)\nprint(y_features.value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T21:40:44.156402Z","iopub.execute_input":"2025-09-08T21:40:44.156724Z","iopub.status.idle":"2025-09-08T21:43:36.074526Z","shell.execute_reply.started":"2025-09-08T21:40:44.156696Z","shell.execute_reply":"2025-09-08T21:43:36.073247Z"}},"outputs":[{"name":"stdout","text":"(10001, 29)\nstructural_breakpoint\n0    7092\n1    2909\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"X_features.sample()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T21:43:42.430686Z","iopub.execute_input":"2025-09-08T21:43:42.430984Z","iopub.status.idle":"2025-09-08T21:43:42.450582Z","shell.execute_reply.started":"2025-09-08T21:43:42.430960Z","shell.execute_reply":"2025-09-08T21:43:42.449706Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"       b_mean   a_mean  b_median  a_median   b_std     a_std     b_iqr  \\\nid                                                                       \n731  0.001303  0.00086  0.000843  0.002584  0.0144  0.016119  0.015806   \n\n        a_iqr  mean_diff  median_diff  ...  acf_lag1_diff  acf_lag2_diff  \\\nid                                     ...                                 \n731  0.019506  -0.000442     0.001742  ...      -0.012684       0.048913   \n\n     acf_lag5_diff  acf_lag10_diff  perm_entropy_diff  ms_entropy_diff  \\\nid                                                                       \n731       0.023514        0.015169            0.00136              0.0   \n\n     lz_complexity_diff  ks_stat  wasserstein  hilbert_mean_diff  \nid                                                                \n731               -41.0  0.07331     0.002299           0.002619  \n\n[1 rows x 29 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>b_mean</th>\n      <th>a_mean</th>\n      <th>b_median</th>\n      <th>a_median</th>\n      <th>b_std</th>\n      <th>a_std</th>\n      <th>b_iqr</th>\n      <th>a_iqr</th>\n      <th>mean_diff</th>\n      <th>median_diff</th>\n      <th>...</th>\n      <th>acf_lag1_diff</th>\n      <th>acf_lag2_diff</th>\n      <th>acf_lag5_diff</th>\n      <th>acf_lag10_diff</th>\n      <th>perm_entropy_diff</th>\n      <th>ms_entropy_diff</th>\n      <th>lz_complexity_diff</th>\n      <th>ks_stat</th>\n      <th>wasserstein</th>\n      <th>hilbert_mean_diff</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>731</th>\n      <td>0.001303</td>\n      <td>0.00086</td>\n      <td>0.000843</td>\n      <td>0.002584</td>\n      <td>0.0144</td>\n      <td>0.016119</td>\n      <td>0.015806</td>\n      <td>0.019506</td>\n      <td>-0.000442</td>\n      <td>0.001742</td>\n      <td>...</td>\n      <td>-0.012684</td>\n      <td>0.048913</td>\n      <td>0.023514</td>\n      <td>0.015169</td>\n      <td>0.00136</td>\n      <td>0.0</td>\n      <td>-41.0</td>\n      <td>0.07331</td>\n      <td>0.002299</td>\n      <td>0.002619</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows √ó 29 columns</p>\n</div>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"X_features.isna().sum().sort_values(ascending=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T21:43:43.854523Z","iopub.execute_input":"2025-09-08T21:43:43.854812Z","iopub.status.idle":"2025-09-08T21:43:43.864884Z","shell.execute_reply.started":"2025-09-08T21:43:43.854792Z","shell.execute_reply":"2025-09-08T21:43:43.863621Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"b_mean                    0\nfft_peak_diff             0\nwasserstein               0\nks_stat                   0\nlz_complexity_diff        0\nms_entropy_diff           0\nperm_entropy_diff         0\nacf_lag10_diff            0\nacf_lag5_diff             0\nacf_lag2_diff             0\nacf_lag1_diff             0\nspectral_centroid_diff    0\nspectral_entropy_diff     0\nfft_domfreq_shift         0\nslope_diff                0\na_mean                    0\nkurt_diff                 0\nskew_diff                 0\nstd_ratio                 0\niqr_diff                  0\nmedian_diff               0\nmean_diff                 0\na_iqr                     0\nb_iqr                     0\na_std                     0\nb_std                     0\na_median                  0\nb_median                  0\nhilbert_mean_diff         0\ndtype: int64"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"### Model Training And Evaluation","metadata":{}},{"cell_type":"code","source":"%%writefile main.py\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T17:39:25.142794Z","iopub.execute_input":"2025-09-08T17:39:25.143186Z","iopub.status.idle":"2025-09-08T17:39:25.151418Z","shell.execute_reply.started":"2025-09-08T17:39:25.143161Z","shell.execute_reply":"2025-09-08T17:39:25.149799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ======= Refactor-friendly train() and infer() =======\nimport os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils.class_weight import compute_sample_weight\nimport matplotlib.pyplot as plt\n\ndef train(X_train: pd.DataFrame, y_train: pd.Series, model_directory_path: str):\n    \"\"\"\n    Train stacking model with XGB as meta learner, 7-fold CV, full class imbalance handling.\n    \"\"\"\n    os.makedirs(model_directory_path, exist_ok=True)\n    rng_seed = 42\n\n    # 1) Feature extraction\n    X_features, y_features = build_all_features(X_train, y_train)\n    print(f\"[train] Features built: {X_features.shape}, labels: {y_features.value_counts().to_dict()}\")\n\n    # 2) Compute positive class ratio for XGB\n    neg = len(y_features) - int(y_features.sum())\n    pos = int(y_features.sum())\n    pos_ratio = neg / (pos + 1e-9)\n\n    # 3) Define base models with full imbalance handling\n    base_models = [\n        (\"xgb_base\", xgb.XGBClassifier(\n            n_estimators=400, learning_rate=0.05, max_depth=5,\n            subsample=0.8, colsample_bytree=0.8,\n            eval_metric=\"logloss\", use_label_encoder=False,\n            random_state=rng_seed, scale_pos_weight=pos_ratio\n        )),\n        (\"rf\", RandomForestClassifier(\n            n_estimators=400, max_depth=12, n_jobs=-1,\n            random_state=rng_seed, class_weight=\"balanced\"\n        )),\n        (\"hgb\", HistGradientBoostingClassifier(\n            max_iter=300, max_depth=5, learning_rate=0.05,\n            random_state=rng_seed, class_weight=\"balanced\"\n        )),\n        (\"mlp\", Pipeline([\n            (\"scaler\", RobustScaler()),\n            (\"mlp\", MLPClassifier(hidden_layer_sizes=(128,), max_iter=500, random_state=rng_seed))\n        ])),\n        (\"logreg\", Pipeline([\n            (\"scaler\", RobustScaler()),\n            (\"logreg\", LogisticRegression(solver=\"liblinear\", class_weight=\"balanced\", random_state=rng_seed))\n        ]))\n    ]\n\n    # 4) Prepare 7-fold GroupKFold\n    gkf = GroupKFold(n_splits=7)\n    groups = X_features.index\n    oof_preds_per_model = {name: np.zeros(len(X_features)) for name, _ in base_models}\n    fold_results = pd.DataFrame(columns=[f\"fold_{i}\" for i in range(1, 8)], index=[name for name, _ in base_models])\n    fold_results.index.name = \"model\"\n\n    calibrated_estimators = []\n\n    # 5) Fit base models with CV and calibration\n    for name, model in base_models:\n        print(f\"[train] CV for base model: {name}\")\n        fold_preds = np.zeros(len(X_features))\n        for fold, (tr_idx, val_idx) in enumerate(gkf.split(X_features, y_features, groups)):\n            X_tr, X_val = X_features.iloc[tr_idx], X_features.iloc[val_idx]\n            y_tr, y_val = y_features.iloc[tr_idx], y_features.iloc[val_idx]\n\n            # Compute sample_weight for MLP\n            if name == \"mlp\":\n                sample_weight = compute_sample_weight(class_weight=\"balanced\", y=y_tr)\n                model.fit(X_tr, y_tr, mlp__sample_weight=sample_weight)\n            else:\n                model.fit(X_tr, y_tr)\n\n            proba = model.predict_proba(X_val)[:, 1]\n            fold_preds[val_idx] = proba\n\n            # Record fold ROC-AUC\n            fold_results.loc[name, f\"fold_{fold+1}\"] = roc_auc_score(y_val, proba)\n            print(f\"   fold {fold} | {name} ROC-AUC: {fold_results.loc[name, f'fold_{fold+1}']:.4f}\")\n\n        oof_score = roc_auc_score(y_features, fold_preds)\n        print(f\"[train] {name} OOF ROC-AUC: {oof_score:.4f}\")\n        oof_preds_per_model[name] = fold_preds.copy()\n\n        # Fit calibrated model on full data\n        calibrated = CalibratedClassifierCV(model, method=\"isotonic\", cv=3)\n        calibrated.fit(X_features, y_features)\n        calibrated_estimators.append((name, calibrated))\n\n    # 6) Meta learner = XGB with imbalance handling\n    meta = xgb.XGBClassifier(\n        n_estimators=300, learning_rate=0.05, max_depth=5,\n        subsample=0.8, colsample_bytree=0.8,\n        eval_metric=\"logloss\", use_label_encoder=False,\n        random_state=rng_seed, scale_pos_weight=pos_ratio\n    )\n\n    # 7) Stacking\n    stack = StackingClassifier(\n        estimators=calibrated_estimators,\n        final_estimator=meta,\n        stack_method=\"predict_proba\",\n        passthrough=True,\n        cv=7,\n        n_jobs=-1\n    )\n\n    # 8) Fit final stacking\n    stack.fit(X_features, y_features)\n    final_oof = roc_auc_score(y_features, stack.predict_proba(X_features)[:, 1])\n    print(f\"[train] Final stacking OOF ROC-AUC: {final_oof:.4f}\")\n\n    # 9) Add final stacking OOF to fold_results\n    fold_results.loc[\"stacking_final\", :] = [final_oof]*fold_results.shape[1]\n    fold_results[\"mean_auc\"] = fold_results.mean(axis=1)\n    print(\"\\n=== Fold ROC-AUC Results ===\")\n    print(fold_results)\n\n    # 10) Plot ROC-AUC per fold\n    plt.figure(figsize=(10,6))\n    plot_df = fold_results.drop(columns=\"mean_auc\")\n    for model in plot_df.index:\n        plt.plot(plot_df.columns, plot_df.loc[model], marker='o', label=model)\n    plt.title(\"ROC-AUC per Fold for Base Models and Stacking\")\n    plt.xlabel(\"Fold\")\n    plt.ylabel(\"ROC-AUC\")\n    plt.ylim(0.5, 1.0)\n    plt.grid(True)\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    # 11) Save artifact\n    artifact = {\n        \"feature_cols\": list(X_features.columns),\n        \"stacking_model\": stack,\n        \"oof_per_model\": oof_preds_per_model,\n        \"train_oof_score\": float(final_oof),\n        \"random_seed\": rng_seed\n    }\n    joblib.dump(artifact, os.path.join(model_directory_path, \"model.joblib\"))\n    print(f\"[train] Saved artifact to {os.path.join(model_directory_path, 'model.joblib')}\")\n\n    \ndef infer(X_test, model_directory_path):\n    \"\"\"\n    Inference function using trained stacking model.\n    - X_test: MultiIndex DataFrame (id,time) with columns ['value','period']\n    - model_directory_path: path to saved model.joblib\n    Returns: DataFrame with 'id', 'pred_proba', 'pred_label'\n    \"\"\"\n\n    import os, joblib, pandas as pd\n    \n    # Load artifact\n    artifact = joblib.load(os.path.join(model_directory_path, \"model.joblib\"))\n    stack_model = artifact[\"stacking_model\"]\n    feature_cols = artifact[\"feature_cols\"]\n\n    # signal ready\n    yield\n\n    if X_test is None:\n        # safety check\n        return\n\n    # process each test DF (single series) and extract features then predict\n    for df in X_test:\n        feats = extract_selected_features_series(df)\n        if feats is None:\n            yield 0.0\n            continue\n        x = pd.DataFrame([feats])\n        x = x.reindex(columns=feature_cols).fillna(0)\n        proba = float(stack.predict_proba(x)[0, 1])\n        yield proba\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T22:14:49.546046Z","iopub.status.idle":"2025-09-08T22:14:49.546346Z","shell.execute_reply.started":"2025-09-08T22:14:49.546220Z","shell.execute_reply":"2025-09-08T22:14:49.546232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, y_train, X_test = crunch.load_data()\ntrain(X_train, y_train, \"artifacts\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T22:10:23.668170Z","iopub.execute_input":"2025-09-08T22:10:23.668513Z","iopub.status.idle":"2025-09-08T22:14:49.545245Z","shell.execute_reply.started":"2025-09-08T22:10:23.668486Z","shell.execute_reply":"2025-09-08T22:14:49.543463Z"}},"outputs":[{"name":"stdout","text":"data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\ndata/X_train.parquet: already exists, file length match\ndata/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\ndata/X_test.reduced.parquet: already exists, file length match\ndata/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\ndata/y_train.parquet: already exists, file length match\ndata/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\ndata/y_test.reduced.parquet: already exists, file length match\n[train] Features built: (10001, 29), labels: {0: 7092, 1: 2909}\n[train] CV for base model: xgb_base\n   fold 0 | xgb_base ROC-AUC: 0.6590\n   fold 1 | xgb_base ROC-AUC: 0.7030\n   fold 2 | xgb_base ROC-AUC: 0.7045\n   fold 3 | xgb_base ROC-AUC: 0.6945\n   fold 4 | xgb_base ROC-AUC: 0.6709\n   fold 5 | xgb_base ROC-AUC: 0.6538\n   fold 6 | xgb_base ROC-AUC: 0.6743\n[train] xgb_base OOF ROC-AUC: 0.6802\n[train] CV for base model: rf\n   fold 0 | rf ROC-AUC: 0.6565\n   fold 1 | rf ROC-AUC: 0.6964\n   fold 2 | rf ROC-AUC: 0.7031\n   fold 3 | rf ROC-AUC: 0.7037\n   fold 4 | rf ROC-AUC: 0.6724\n   fold 5 | rf ROC-AUC: 0.6818\n   fold 6 | rf ROC-AUC: 0.6959\n[train] rf OOF ROC-AUC: 0.6872\n[train] CV for base model: hgb\n   fold 0 | hgb ROC-AUC: 0.6492\n   fold 1 | hgb ROC-AUC: 0.6878\n   fold 2 | hgb ROC-AUC: 0.6992\n   fold 3 | hgb ROC-AUC: 0.6856\n   fold 4 | hgb ROC-AUC: 0.6729\n   fold 5 | hgb ROC-AUC: 0.6541\n   fold 6 | hgb ROC-AUC: 0.6678\n[train] hgb OOF ROC-AUC: 0.6743\n[train] CV for base model: mlp\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2426602407.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"artifacts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/2020112046.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, y_train, model_directory_path)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mlp\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp__sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: BaseMultilayerPerceptron.fit() got an unexpected keyword argument 'sample_weight'"],"ename":"TypeError","evalue":"BaseMultilayerPerceptron.fit() got an unexpected keyword argument 'sample_weight'","output_type":"error"}],"execution_count":43},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_fold_roc(fold_results):\n    plt.figure(figsize=(10,6))\n\n    # Exclude mean column\n    plot_df = fold_results.drop(columns=\"mean_auc\")\n\n    for model in plot_df.index:\n        plt.plot(plot_df.columns, plot_df.loc[model], marker='o', label=model)\n\n    plt.title(\"ROC-AUC per Fold for Base Models and Stacking\")\n    plt.xlabel(\"Fold\")\n    plt.ylabel(\"ROC-AUC\")\n    plt.ylim(0.5, 1.0)\n    plt.grid(True)\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n\n# After printing fold_results\nplot_fold_roc(fold_results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = list(infer(X_test, \"artifacts\"))\nprint(preds[:10])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T17:47:54.840152Z","iopub.execute_input":"2025-09-08T17:47:54.841013Z","iopub.status.idle":"2025-09-08T17:48:21.578777Z","shell.execute_reply.started":"2025-09-08T17:47:54.840977Z","shell.execute_reply":"2025-09-08T17:48:21.577897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"crunch.test(\n    # Uncomment to disable the train\n    #force_first_train=False,\n\n    # Uncomment to disable the determinism check\n    # no_determinism_check=True,\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T17:49:42.846254Z","iopub.execute_input":"2025-09-08T17:49:42.846590Z","iopub.status.idle":"2025-09-08T17:58:37.938237Z","shell.execute_reply.started":"2025-09-08T17:49:42.846566Z","shell.execute_reply":"2025-09-08T17:58:37.937015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction = pd.read_parquet(\"data/prediction.parquet\")\n\nprediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T18:20:17.203721Z","iopub.execute_input":"2025-09-08T18:20:17.204255Z","iopub.status.idle":"2025-09-08T18:20:17.220073Z","shell.execute_reply.started":"2025-09-08T18:20:17.204225Z","shell.execute_reply":"2025-09-08T18:20:17.219116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the targets\ntarget = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n\n# Call the scoring function\nsklearn.metrics.roc_auc_score(\n    target,\n    prediction,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T18:20:34.762252Z","iopub.execute_input":"2025-09-08T18:20:34.762588Z","iopub.status.idle":"2025-09-08T18:20:34.775879Z","shell.execute_reply.started":"2025-09-08T18:20:34.762563Z","shell.execute_reply":"2025-09-08T18:20:34.775045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T18:49:24.357963Z","iopub.execute_input":"2025-09-08T18:49:24.358973Z","iopub.status.idle":"2025-09-08T18:49:24.368941Z","shell.execute_reply.started":"2025-09-08T18:49:24.358895Z","shell.execute_reply":"2025-09-08T18:49:24.367970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}